{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "# Videos and Exercises for Session 10: Introduction to Modelling and Machine Learning\n",
    "\n",
    "In this notebook, you will mainly be working with the Perceptron model. First, you will be introduced to theoretical aspects and applications for the model $-$ then you will be asked to produce a version of it yourself in a series of smaller steps. \n",
    "\n",
    "This part might take many of you some time to code up, but it is worth the effort. Not only should it help you increase your understanding of basic machine learning machinery. It should also be helpful for assignment 2 :) \n",
    "\n",
    "Part 2 mainly consists of a series of bonus exercises where you will be working with Adaline $-$ an extension of the perceptron. If you do not have time to solve the exercises, make sure to watch the videos! \n",
    "\n",
    "The structure is as follows:\n",
    "1. The Perceptron Model\n",
    "    - Implementing and Using the Model in Python\n",
    "    - Validation of a Model\n",
    "2. Beyond the Perceptron Model\n",
    "    - Logistic Regression\n",
    "    - Adaline\n",
    "    \n",
    "**NOTE:** I may be speaking quite slowly in some of the videos. A good advice is to turn up the speed to x1.25 or x1.5 if you want to get through without spending too much time:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: The Perceptron Model\n",
    "\n",
    "The first supervised learning model that we will introduce is an old model. We will learn about it because it is simple enough to grasp how it works, and we will use to build the intuition for more advanced models. The video below introduces the model theoretically with mathematics. \n",
    "\n",
    "Parts of the talk will use matrices to make computations. Thus, you may want to re-familiarize yourself with [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication) before starting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('C1WF4MWEQnQ', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing and Using the Model in Python\n",
    "\n",
    "We now implement show you how the Perceptron model can be implemented in Python. First, we show you how you can initialize parameters. Then we show you how you can compute errors and update weights in a _vectorized_ way. As you may remember from the previous video, the Perceptron weights are typically updated one observation at a time - and this is also how you will implement it in a simple version below :). However, most alternative algorithms will use vectorized errors (for instance Adaline further down) on either the full set of data or a subset of data, which is why we show this in the video.\n",
    "\n",
    "The video also shows how we can use others' code - in this case, we use Raschka's implementation of the Perceptron. You can see where Raschka's code is loaded by checking out the slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('ySA6QTDX7CA', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of a Model\n",
    "\n",
    "We want to have a credible measure of model performance. In this video, we talk about a simple approach to getting such a measure for cross-section/static data (i.e. not time series)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('UtF-KitNHtY', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> **Ex. 10.1.1:** The mathematics and biological reasoning which justifies the perceptron model is presented in Raschka, 2017 on pages 18 to 24. If you haven't read it already, quickly do so. \n",
    ">\n",
    "> Begin by importing `numpy`, `pandas` and `seaborn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34f93fe2b73cb28e99c5efd31a15b13f",
     "grade": false,
     "grade_id": "cell-83b57b9149cb54a2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.1.2:** Use the following code snippet to load the iris data. The code will create two new variablex **X** and **y**, each of which are numpy arrays. Split the data as follows:\n",
    "1. The first dataset should contain the first 70 rows; we call this sample our *training dataset*, or simply *train data* (`Xtrain` and `ytrain`). We use the training data to estimate the data. \n",
    "2. We use the remaining rows as data for testing our model, thus we call it *test data* (`Xtest` and `ytest`). \n",
    ">\n",
    ">```python \n",
    "iris = sns.load_dataset('iris')\n",
    "iris = iris.query(\"species == 'virginica' | species == 'versicolor'\").sample(frac=1, random_state = 3)\n",
    "X = np.array(iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']])\n",
    "y = np.array(iris['species'].map({'virginica': 1, 'versicolor': -1}))\n",
    "sns.pairplot(iris, hue=\"species\", palette=\"husl\", diag_kws = {'shade': False})\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5f974f5113601ea0f4439f43bbc91f3",
     "grade": false,
     "grade_id": "cell-44861170623379b8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.1.3:** Write a function which initiate a set of weights `W` with *length* that is one larger than the number of features in your data. Ensure that your initial weights are not exactly 0, but close to it. \n",
    ">\n",
    ">> _Hint 1:_ Use [np.random.RandomState](https://numpy.org/doc/1.16/reference/generated/numpy.random.RandomState.html) to set up a random number generator from which you can draw from a normal with mean 0 and scale 0.01. \n",
    ">\n",
    ">> _Hint 2:_ Say you have stored the random number generator in an object called `rgen`. You can then call `rgen.normal(size = 1 + columns_in_X)` to get the weights you want. You might want to tweak the `scale` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e3b503614aff204d3f433856c18db54",
     "grade": false,
     "grade_id": "cell-2f566125986ebf20",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.1.4:** In this problem, you need to write two functions:\n",
    "> * `net_input(X, W)`: calculates _and returns_ the net-input, i.e the linear combination of features and weights, $z=w_0 + \\sum_k x_{k} w_{k}$\n",
    "> * `predict(X, W)`: a step function which returns 1 if the net activation is $\\geq$ 0, and returns -1 otherwise. \n",
    ">\n",
    ">*Bonus:* Create a function which calculates the _accuracy_ (the share of cases that are correctly classified). The function should take a vector of y-values and a vector of predicted y-values as input. What is the accuracy of your untrained model on the training data?\n",
    "\n",
    "> _Hint 1:_ you can compute the above using an array product. Here numpy's array product named `dot` may be useful \n",
    "\n",
    "> _Hint 2:_ remember to include the bias, $w_0$, in the computation!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad5e3997c561b73c4f881d23f72cc33a",
     "grade": false,
     "grade_id": "cell-9a4c65944f10d113",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.1.5:** Write a function whichs loops over the training data (both X and y) using `zip`. For each row in the data, update the weights according to the perceptron rule (remember to update the bias in `w[0]`!). Set $\\eta = 0.1$.\n",
    ">\n",
    "> Make sure the loop stores the total number of prediction errors encountered underways in the loop by creating an `int` which is incremented whenever you update the weights. \n",
    ">\n",
    ">> _Hint:_ your function should return the updated weights, as well as the number of errors made by the perceptron.\n",
    ">\n",
    ">> _Hint:_ The following code block implements the function in _pseudo_code (it wont run, but serves to communicate the functionality).\n",
    ">> ```\n",
    ">> function f(X, y, W, eta):\n",
    ">>    set errors = 0\n",
    ">>\n",
    ">>    for each pair xi, yi in zip(X,y) do:\n",
    ">>        set update = eta * (yi - predict(xi, W))\n",
    ">>        set W[1:] = W[1:] + update * xi\n",
    ">>        set W[0] = W[0] + update\n",
    ">>        set errors = errors + int(update != 0) \n",
    ">>\n",
    ">>    return W, errors\n",
    ">> ```\n",
    ">\n",
    "> *Bonus:* If you completed the previous bonus exercise (for 10.1.4), calculate the accuracy on training data using the updated weights as input in the predict function. Any progress yet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCLUDED IN ASSIGNMENT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7eaaaa47b55e584c64d865d8e5b7d751",
     "grade": false,
     "grade_id": "cell-67d2591f252b5e1b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.1.6:** Write a function, which repeats the updating procedure (calls the function) you constructed in 10.1.5 for `n_iter` times by packing the whole thing in a loop. Make sure you store the number of errors in each iteration in a list. \n",
    ">\n",
    "> Plot the total errors after each iteration in a graph.\n",
    ">\n",
    "> _Hint 1:_ Make sure you dont reset the weights after each iteration.\n",
    ">\n",
    "> _Hint 2:_ Once again some pseudocode:\n",
    ">> ```\n",
    ">> function g(X, y, n_iter):\n",
    ">>     set eta = 0.1\n",
    ">>     set weights = random_weights()\n",
    ">>     set errorseq = list()\n",
    ">>\n",
    ">>     for each _ in range(n_iter):\n",
    ">>         weights, e = f(X, y, weights, eta) \n",
    ">>         errorseq.append(e)\n",
    ">>\n",
    ">>     return weights, errorseq\n",
    ">> ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCLUDED IN ASSIGNMENT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "724329828010bf30230618b24583db0b",
     "grade": false,
     "grade_id": "cell-059f1639214a2ed7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.1.7 (BONUS):** Use the updated weights when predicting and calculate the accuracy of your perceptron on the test data?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12285bf4fc19c433a34e05d3eeb850b8",
     "grade": false,
     "grade_id": "cell-e39721b77e2e76f9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex.10.1.8 (BONUS):** Restructure your code as a class called `Perceptron` with `.fit()` and `.predict()` methods (you) will probably need more helper methods. Store hyperparameters as eta and the number of iterations as class attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd73201b91fa1636839bc84f9b242a11",
     "grade": false,
     "grade_id": "cell-927f8385220c1a89",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Perceptron(X = Xtrain, y= ytrain).fit()\n",
    "plt.plot(p._errseq, 'b-o') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: Beyond the Perceptron Model\n",
    "\n",
    "Having seen and worked with the perceptron, we now want to provide you with some ideas on how we can change parts of the perceptron to obtain another model. In particular, we will introduce the _logistic regression model_ and the _adaline (adaptive linear neuron) model_. \n",
    "\n",
    "In practice, these models distinguish themselves along particularly two margins. \n",
    "- First, in the standard implementation, weight updating is typically conducted using (a batch of the) full training data set - rather than one observation at a time. \n",
    "- Furthermore, weight updating essentially accounts for the _degree_ at which we are making a wrong prediction. \n",
    "    - If we are very certain that observation _i_ is the positive type, when it is in fact negative, we will treat it differently from an observation that we are not very certain of when misclassifying. \n",
    "    - As we will see, logistic regressions, is also based on a notion of _conditional probabilites_ (i.e. \"what is the probability that _i_ is the positive type when _x_ is equal to 5), when while Adaline is not.\n",
    "\n",
    "*Note:* Again, you may want to familiarize yourself with background concepts: [gradient](https://en.wikipedia.org/wiki/Gradient), [sum of squared errors](https://en.wikipedia.org/wiki/Residual_sum_of_squares) and the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('nUCj4QdWpCM', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is another simple linear machine-learning algorithm, you can read about it [here:](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.0:** Import the LogisticRegression classifier from `sklearn.linear_model`. Create a new object called `clf` like:\n",
    "> ```\n",
    "> clf = LogisticRegression()\n",
    "> ```\n",
    "All scikit learn models have two fundamental methods `.fit()` and `.predict()`. Fit your model to the training data, and store the fitted model in a new object. Import _accuracy_score_ from `sklearn.metrics` and asses the accuracy of the LogisticRegression on both your training data and your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e245ccf10e5687f828736146707bb51",
     "grade": false,
     "grade_id": "cell-64462b79a8d3cc8a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaLine\n",
    "As we saw in the video, AdaLine is a modified version of the perceptron. The most important difference lies in the way the two models learn from their training data, i.e. the optimization method used. The perceptron used the binary classifications for learning, while AdaLine only applies the binary threshold after training, and thus uses real valued numbers when learning. Another difference is that in the standard implementation, the weight update in Adaline is conducted using the full set of data or a subset of data.\n",
    "\n",
    "> _Hint:_ In this set of exercises, most of the code for the following exercise can be written by copying and modifying code from previous exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.1:** Implement two functions as described below. You shold reuse your `net_input` from previous exercises:\n",
    "> * `ada_activation_function`: the linear function $ada\\_activation(z) = z$\n",
    "> * `ada_predict`: A step function   $ada\\_predict(z) = 1 \\ if \\ z \\geq 0  \\ else \\ 0$ where z is the output of _the activation function_.\n",
    "\n",
    "\n",
    "\n",
    "> The following figure might help you understand how each of these functions relate to the algorithm, and how the perceptron and adaline differ:\n",
    "![asd](https://sebastianraschka.com/images/faq/diff-perceptron-adaline-neuralnet/4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60e7fb372d0acaba9f87c5708fdd1231",
     "grade": false,
     "grade_id": "cell-8e63b21a623d6fda",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.2:** AdaLine uses a _cost function_ to quantize the accuracy of the classifier this is given by \n",
    ">$$ \n",
    "cost(X,y,W) = \\frac{1}{2} \\sum_{i=1}^N (y_i - activation(z_i) )^2 , \\qquad z_i = net\\_input(x_i, W)\n",
    "$$\n",
    "> If you've followed any normal undergraduate courses in statistics you should recognize this function. Begin by implementing the cost function. Unlike in undergraduate statistics we will optimize our estimator using gradient descent, therefore **code up the negative of the derivative of the cost function as well**. \n",
    "> $$ \n",
    "-cost'_j(X,y, W) = -\\sum_{i=1}^N (y_i - activation(z_i)) x_i^j,  \\qquad z_i = net\\_input(x_i, W)\n",
    "$$\n",
    ">\n",
    ">> _Hint:_ Dont compute the sum for each weight $w_j$, instead use numpy's matrix algebra to compute the all of the derivatives at once.\n",
    ">\n",
    ">> _Hint:_ The derivative should return a list of the same length as the number of weights, since there is one derivative for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c55fe6efee197cdf1e28df4214527593",
     "grade": false,
     "grade_id": "cell-b5b1cd22ca65a2fd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.3:** Implement the adaline fitting algorithm using *batch gradient descent*. This is similar to what you did with the perceptron, but while the perceptron did it's optimization after evaluating each row in the dataset, adaline treats the entire dataset as a batch, adjusts it's weights and then does it all again. Thus you only need to loop over `n_iter`, _not_ the data rows. Use the cost function to track the progress of your algorithm.\n",
    ">\n",
    "> _Hint:_ gradient descent will be extremely sensitive to the learning rate $\\eta$ in this situation - try setting i to 0.0001 and running the algorithm for 5000 iterations to get some kind of convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06a154b0507f95fcc55c65ad42be6de8",
     "grade": false,
     "grade_id": "cell-ce729986f2f1b230",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.4:** Write a function that scales each of the variables in the dataset (including **y**) using the formula \n",
    "$$\n",
    "x_j^{new} = \\frac{x_j^{old} - \\mu_j}{\\sigma_j}\n",
    "$$\n",
    "> rerun the adaline function on the scaled variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56e621f7a56e3efef9a18da453ea6ea9",
     "grade": false,
     "grade_id": "cell-4698ea41d9f80069",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
