{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **DO YOU USE GITHUB?**  \n",
    "If True: print('Remember to make your edits in a personal copy of this notebook')  \n",
    "Else: print('You don't have to understand. Continue your life.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8: Web Scraping 3\n",
    "\n",
    "Now you know how to investigate, download and parse data from a website. Thank you module_6 and module_7. Though, you might still run into websites you are having a difficult time scraping. Getting thorugh such cahllenge using automated browsing will be the main topic of this module. \n",
    "\n",
    "We will continue to learn new techniques of parsing unstructured text and HTML. This will help you build ***custom datasets*** within just a few hours or days work, that would have taken ***months*** to curate and clean manually. We will learn how to automate browsing and use regex to parse text without html tags.\n",
    "\n",
    "\n",
    "Readings for `module 6+7+8`:\n",
    "- [Python for Data Analysis, chapter 6](https://bedford-computing.co.uk/learning/wp-content/uploads/2015/10/Python-for-Data-Analysis.pdf)\n",
    "- [A Practical Introduction to Web Scraping in Python](https://realpython.com/python-web-scraping-practical-introduction/)\n",
    "- [An introduction to web scraping with Python](https://towardsdatascience.com/an-introduction-to-web-scraping-with-python-a2601e8619e5)\n",
    "- [Introduction to Web Scraping using Selenium](https://medium.com/the-andela-way/introduction-to-web-scraping-using-selenium-7ec377a8cf72)\n",
    "\n",
    "Video materiale from `ISDS 2020`:\n",
    "- [Web Scraping 1](https://bit.ly/ISDS2021_6)\n",
    "- [Web Scraping 2](https://bit.ly/ISDS2021_7)\n",
    "- [Web Scraping 3](https://bit.ly/ISDS2021_8)\n",
    "\n",
    "Other ressources:\n",
    "- [Nicklas Webpage](https://nicklasjohansen.netlify.app/)\n",
    "- [Data Driven Organizational Analysis, Fall 2021](https://efteruddannelse.kurser.ku.dk/course/2021-2022/ASTK18379U)\n",
    "- [Master of Science (MSc) in Social Data Science](https://www.socialdatascience.dk/education)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions and Automated Browsing\n",
    "Sometimes scraping tasks demand interactions (e.g. login, scrolling, clicking), and a no XHR data can be found easily, so you need the browser to execute the scripts before you can get the data. XHR is short for XMLHttpRequest - a JavaScript API - like the one we found in the jobnet.dk exerise.\n",
    "\n",
    "Here we use the `Selenium` package in combination with the `ChromeDriver` - you can download the latest release [here](https://chromedriver.chromium.org/downloads). It allows you to animate a browser. \n",
    "\n",
    "Make sure to download the driver as well as the newest version of Selenium. \"pip install selenium\" should do the trick. \n",
    "\n",
    "Some developers prefer to you [geckodriver](https://github.com/mozilla/geckodriver/releases) as an alternative to `ChromeDriver`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 92.0.4515\n",
      "Get LATEST driver version for 92.0.4515\n",
      "Driver [C:\\Users\\danib\\.wdm\\drivers\\chromedriver\\win32\\92.0.4515.107\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "url = 'https:google.com'\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SessionNotCreatedException",
     "evalue": "Message: session not created: This version of ChromeDriver only supports Chrome version 93\nCurrent browser version is 92.0.4515.131 with binary path C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSessionNotCreatedException\u001b[0m                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-9e75b312aef1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'chromedriver'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecutable_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options, keep_alive)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             RemoteWebDriver.__init__(\n\u001b[0m\u001b[0;32m     77\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m                 command_executor=ChromeRemoteConnection(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, command_executor, desired_capabilities, browser_profile, proxy, keep_alive, file_detector, options)\u001b[0m\n\u001b[0;32m    155\u001b[0m             warnings.warn(\"Please use FirefoxOptions to set browser profile\",\n\u001b[0;32m    156\u001b[0m                           DeprecationWarning, stacklevel=2)\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcapabilities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrowser_profile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_switch_to\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSwitchTo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mobile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMobile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mstart_session\u001b[1;34m(self, capabilities, browser_profile)\u001b[0m\n\u001b[0;32m    250\u001b[0m         parameters = {\"capabilities\": w3c_caps,\n\u001b[0;32m    251\u001b[0m                       \"desiredCapabilities\": capabilities}\n\u001b[1;32m--> 252\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNEW_SESSION\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'sessionId'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'value'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSessionNotCreatedException\u001b[0m: Message: session not created: This version of ChromeDriver only supports Chrome version 93\nCurrent browser version is 92.0.4515.131 with binary path C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\n"
     ]
    }
   ],
   "source": [
    "# You can also download the driver to your computer\n",
    "# Save it in your working directory and write the code\n",
    "\n",
    "import os\n",
    "directory = os.getcwd()\n",
    "path = os.path.join(directory, 'chromedriver')\n",
    "driver = webdriver.Chrome(executable_path=path)\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: nboards.dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 92.0.4515\n",
      "Get LATEST driver version for 92.0.4515\n",
      "Driver [C:\\Users\\danib\\.wdm\\drivers\\chromedriver\\win32\\92.0.4515.107\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "# step 1: load the webpage we want to scrape in our virtual browser\n",
    "url = 'https://nboard.dk/search'\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benifits from autoamting browsing\n",
    "1. You can access data that are not directly in the HTML code but that is being generating while browsing\n",
    "2. You can get thorugh login screens and other scraping barriers\n",
    "3. You can automate browsing behaviour such as scrolling down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 92.0.4515\n",
      "Get LATEST driver version for 92.0.4515\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='chromedriver.storage.googleapis.com', port=443): Max retries exceeded with url: /LATEST_RELEASE_92.0.4515 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001DCCFFB3970>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m             conn = connection.create_connection(\n\u001b[0m\u001b[0;32m    170\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    917\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1009\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sock\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1010\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1011\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[1;31m# Add certificate verification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSocketError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             raise NewConnectionError(\n\u001b[0m\u001b[0;32m    182\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Failed to establish a new connection: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x000001DCCFFB3970>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m             retries = retries.increment(\n\u001b[0m\u001b[0;32m    756\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    573\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='chromedriver.storage.googleapis.com', port=443): Max retries exceeded with url: /LATEST_RELEASE_92.0.4515 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001DCCFFB3970>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-a7ac5d355ec7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://nboard.dk/search'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mChromeDriverManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\webdriver_manager\\chrome.py\u001b[0m in \u001b[0;36minstall\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minstall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Current {self.driver.chrome_type} version is {self.driver.browser_version}\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_line\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mdriver_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_driver_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchmod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0o755\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\webdriver_manager\\manager.py\u001b[0m in \u001b[0;36m_get_driver_path\u001b[1;34m(self, driver)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mdriver_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mos_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_os_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mdriver_version\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         binary_path = self.driver_cache.find_driver(browser_version, driver_name, os_type,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\webdriver_manager\\driver.py\u001b[0m in \u001b[0;36mget_version\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mdriver_version\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_version\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdriver_version\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"latest\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_latest_release_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_version\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\webdriver_manager\\driver.py\u001b[0m in \u001b[0;36mget_latest_release_version\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_latest_release_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Get LATEST driver version for {self.browser_version}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self._latest_release_url}_{self.browser_version}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[0mvalidate_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    514\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='chromedriver.storage.googleapis.com', port=443): Max retries exceeded with url: /LATEST_RELEASE_92.0.4515 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001DCCFFB3970>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))"
     ]
    }
   ],
   "source": [
    "# step 2: scroll down the page to load more profiles\n",
    "import time\n",
    "\n",
    "url = 'https://nboard.dk/search'\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "for i in range(5):\n",
    "    time.sleep(3)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 92.0.4515\n",
      "Get LATEST driver version for 92.0.4515\n",
      "Driver [C:\\Users\\danib\\.wdm\\drivers\\chromedriver\\win32\\92.0.4515.107\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 27.25 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# step 3: save the soup and keep track of runtime\n",
    "\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "url = 'https://nboard.dk/search'\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(url)\n",
    "\n",
    "for i in range(5):\n",
    "    time.sleep(3)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "print(\"--- %s seconds ---\" % round((time.time() - start_time),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 92.0.4515\n",
      "Get LATEST driver version for 92.0.4515\n",
      "Driver [C:\\Users\\danib\\.wdm\\drivers\\chromedriver\\win32\\92.0.4515.107\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 145.99 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# step 3: save the soup and keep track of runtime\n",
    "\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "url = 'https://nboard.dk/search'\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "lenOfPage = driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "match=False\n",
    "while(match==False):\n",
    "    lastCount = lenOfPage\n",
    "    time.sleep(1)\n",
    "    lenOfPage = driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "    if lastCount==lenOfPage:\n",
    "        match=True\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "print(\"--- %s seconds ---\" % round((time.time() - start_time),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1320\n",
      "https://nboard.dk/candidate_profile/Tobias-Hyldeborg\n"
     ]
    }
   ],
   "source": [
    "# step 4: use the soup to generate our mapping of urls (profiles) that we want to scrape\n",
    "\n",
    "names = soup.find_all('span', {'class': 'name'})\n",
    "\n",
    "urls = []\n",
    "for i in range(len(names)):\n",
    "    temp = 'https://nboard.dk/candidate_profile/'+ str(names[i].text)\n",
    "    temp = temp.replace(' ','-')\n",
    "    temp = temp.replace('--','-')\n",
    "    urls.append(temp)\n",
    "\n",
    "print(len(urls))\n",
    "print(urls[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 8.06 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>location</th>\n",
       "      <th>resume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sven Jensen</td>\n",
       "      <td>International salgsprofil m. bestyrelses erfaring</td>\n",
       "      <td>Sønderborg, Danmark</td>\n",
       "      <td>Jeg har funderet, bred erfaring med eksport sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jens Kofoed</td>\n",
       "      <td>Bestyrelseserfaring, finansielt  &amp; strategisk ...</td>\n",
       "      <td>København, Danmark</td>\n",
       "      <td>Kompetencer:\\nBestyrelseserfaring, strategi- o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tobias Hyldeborg</td>\n",
       "      <td>Digital Strateg Med Speciale I SEO, SEA &amp; Affi...</td>\n",
       "      <td>København, Danmark</td>\n",
       "      <td>Jeg har en bred forståelse for marketing og st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jan Thiim</td>\n",
       "      <td>COO med 15 års ekspertise CX, Digitale strateg...</td>\n",
       "      <td>København, Danmark</td>\n",
       "      <td>Kundefokuseret COO som har fingeren på pulsen....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Titti Kopp</td>\n",
       "      <td>Advokat - erfaring med store bygge- og anlægsp...</td>\n",
       "      <td>København, Danmark</td>\n",
       "      <td>Jeg er advokat og ph.d. med 20 års erfaring i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Peter A. Wistoft</td>\n",
       "      <td>Forretningsorienteret leder -  finansiel speci...</td>\n",
       "      <td>Skanderborg, Danmark</td>\n",
       "      <td>Forretningsorienteret CFO som evner at lede og...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Francesco Pesci</td>\n",
       "      <td>C-suite sales and general management executive</td>\n",
       "      <td>København, Danmark</td>\n",
       "      <td>Executive with more than ten years in senior m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name                                           subtitle  \\\n",
       "0       Sven Jensen  International salgsprofil m. bestyrelses erfaring   \n",
       "1       Jens Kofoed  Bestyrelseserfaring, finansielt  & strategisk ...   \n",
       "2  Tobias Hyldeborg  Digital Strateg Med Speciale I SEO, SEA & Affi...   \n",
       "3         Jan Thiim  COO med 15 års ekspertise CX, Digitale strateg...   \n",
       "4        Titti Kopp  Advokat - erfaring med store bygge- og anlægsp...   \n",
       "5  Peter A. Wistoft  Forretningsorienteret leder -  finansiel speci...   \n",
       "6   Francesco Pesci     C-suite sales and general management executive   \n",
       "\n",
       "               location                                             resume  \n",
       "0   Sønderborg, Danmark  Jeg har funderet, bred erfaring med eksport sa...  \n",
       "1    København, Danmark  Kompetencer:\\nBestyrelseserfaring, strategi- o...  \n",
       "2    København, Danmark  Jeg har en bred forståelse for marketing og st...  \n",
       "3    København, Danmark  Kundefokuseret COO som har fingeren på pulsen....  \n",
       "4    København, Danmark  Jeg er advokat og ph.d. med 20 års erfaring i ...  \n",
       "5  Skanderborg, Danmark  Forretningsorienteret CFO som evner at lede og...  \n",
       "6    København, Danmark  Executive with more than ten years in senior m...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 5: scraping profiles \n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "name = []\n",
    "subtitle = []\n",
    "location = []\n",
    "resume = []\n",
    "\n",
    "for i in range(7): #len(urls)\n",
    "    response = requests.get(urls[i])\n",
    "    html = response.text\n",
    "    \n",
    "    if 'Internal server error' in html:\n",
    "        continue\n",
    "    \n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    name.append(soup.find('title').text)\n",
    "    subtitle.append(soup.find('span', {'class': 'sub-title'}).text)\n",
    "    location.append(soup.find('span', {'class': 'location'}).text)\n",
    "    resume.append(soup.find('span', {'class': 'resume'}).text)\n",
    "\n",
    "df = pd.DataFrame({'name':name, \n",
    "                   'subtitle':subtitle, \n",
    "                   'location':location, \n",
    "                   'resume':resume})\n",
    "\n",
    "print(\"--- %s seconds ---\" % round((time.time() - start_time),2))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next level scrapers\n",
    "\n",
    "You have know learned some of the fundamentals of collecting and parsing data and should be well suited for your exam project. Though I find it important to adress that you might run into some challenges that we have not learn dealing with yet. Facebook, LinkedIn, Google and all the other big tech firms are battling scrapers and has done all kinds of thing to make it hard for us to steal public data on their sites. I have found som article that you might find interessting.\n",
    "\n",
    "- [Most Commonly used techniques to Prevent Scraping:](https://medium.com/@betoayesa/using-the-content-as-an-anti-scrape-weapon-draft-9bb10cd30e5c)\n",
    "- [Advanced Web Scraping Tactics](https://www.pluralsight.com/guides/advanced-web-scraping-tactics-python-playbook)\n",
    "- [Scraping Sites That Use JavaScript and AJAX](https://oup-arc.com/protected/files/content/file/1505319833942-CH9---Scraping-Sites-that-Use-JavaScript-and-AJAX.pdf)\n",
    "- [Get Started Scraping LinkedIn With Python and Selenium](https://medium.com/nerd-for-tech/linked-in-web-scraper-using-selenium-15189959b3ba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting patterns from Raw Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Standard string operations**\n",
    "- `string.split`\n",
    "- `string.strip`\n",
    "- `string.replace`\n",
    "\n",
    "**Regex**\n",
    "\"A regular expression (shortened as regex) is a sequence of characters that define a search pattern. Usually such patterns are used by string-searching algorithms for \"find\" or \"find and replace\" operations on strings, or for input validation.\n",
    "\n",
    "**Examples**\n",
    "- extract currency and amount from raw text: $ 20, 10.000 dollars 10,000 £\n",
    "- email addresses: here you want to design a pattern (as above), that captures only the uses of @ within an email.\n",
    "- urls. Here you are trying to define all the different ways of writing urls (https, http, no http). \n",
    "- Dates. Again many variations: 17th of June 2017, 06/17/17 or 17. June 17\n",
    "- addresses, \n",
    "- phone numbers: 8888888 or 88 88 88 88 or +45 88 88 88 88,\n",
    "- emojiies in text. Capturing all the different ways of expressing smiley faces with one regular expression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nj@sodas.ku.dk'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "line = \"should we use regex more often? let me know at nj@sodas.ku.dk\"\n",
    "match = re.search(r'[\\w\\.-]+@[\\w\\.-]+', line)\n",
    "match.group(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ressources**\n",
    "- Community and interactive playground [here](http://regexr.com/)\n",
    "- Interactive tutorial [here](https://regexone.com/)\n",
    "- Lookup all special characters [here](https://www.regular-expressions.info/refquick.html)\n",
    "\n",
    "**Syntax for build your own expresions**\n",
    "* \\+ = 1 or more times  -- e.g. \"a+\" will match: \"a\", and \"aaa\"\n",
    "* \\* = 0 or more times  -- e.g. \"ba*\" will match: \"b\", and \"ba\", and \"baaa\"\n",
    "* {3} = exactly three times --- e.g. \"ba{3}\" will match \"baaa\", but not \"baa\"\n",
    "* ? = once or none\n",
    "* \\\\ = escape character, used to find characters that has special meaning with regex: e.g. \\+ \\*\n",
    "* [] = allows you to define a set of characters\n",
    "* ^ = applied within a set, it becomes the inverse of the set defined. Applied outside a set it entails the beginning of a string. $ entails the end of a string.\n",
    "* . = any characters except line break\n",
    "* | = or statement. -- e.g. a|b means find characters a or b.\n",
    "* \\d = digits\n",
    "* \\D = any-non-digits.\n",
    "* \\s = whitespace-separator\n",
    "\n",
    "Sequences\n",
    "* (?:) = Defines a Non-capturing group. -- e.g. \"(?:abc)+\", will match \"abc\" and \"abcabcabc\", but not \"aabbcc\"\n",
    "* (?=)\t= Positive lookahead - only match a certain pattern if a certain pattern comes after it.\n",
    "* (?!)\t= Negative lookahead - only match a certain pattern if **not** a certain pattern comes after it.\n",
    "* (?<=)\t= Positive lookbehind - only match a certain pattern if a certain pattern precedes it.\n",
    "* (?<!) = Negative lookbehind - only match a certain pattern if **not** a certain pattern precedes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/snorreralund/scraping_seminar/master/danish_review_sample.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "digit_re = re.compile('[0-9]+') # compiled regular expression for matching digits\n",
    "df['hasNumber'] = df.reviewBody.apply(lambda x: len(digit_re.findall(x))>0) # check if it has a number\n",
    "sample_string = '\\n'.join(df[df.hasNumber].sample(2).reviewBody)\n",
    "#sample_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing expressions**  \n",
    "Our SODAS collegaue Snorre has developed his own regex library for python. You can either use it by copying his entire class \"ExploreRegex\" into your notebook or by following this peace of code:\n",
    "```python \n",
    "# download module\n",
    "url = 'https://raw.githubusercontent.com/snorreralund/explore_regex/master/explore_regex.py'\n",
    "response = requests.get(url)\n",
    "# write script to your folder to create a locate module\n",
    "with open('explore_regex.py','w') as f:\n",
    "    f.write(response.text)\n",
    "# import local module\n",
    "import ExploreRegex\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/snorreralund/explore_regex/blob/master/explore_regex.py\n",
    "\n",
    "import networkx as nx\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def get_span_overlap(span,span2):\n",
    "        return min([span[1],span2[1]])-max([span[0],span2[0]])\n",
    "\n",
    "class ExploreRegex():\n",
    "    \"\"\"This module should allow you to compare the differences in matches between regular expressions.\n",
    "    Default Flags are by design RE.UNICODE and re.DOTALL.\n",
    "    \"\"\"\n",
    "    def __init__(self,sample_string,flags=re.DOTALL|re.UNICODE):\n",
    "        self.string = sample_string\n",
    "        self.patterns = []\n",
    "        self.pattern2span = [] # this container stores all the matches of each pattern.\n",
    "        self.span_graph = nx.Graph() # defines network that\n",
    "        self.span2span = nx.Graph()\n",
    "        self.pattern2pattern = {}\n",
    "        self.pattern2pattern_soft = {}\n",
    "        self.pattern2n_match = {}\n",
    "        self.pattern2chars_matched = {}\n",
    "        self.pattern2idx = {}\n",
    "        self.pattern_comparisons = set()\n",
    "        self.similarity_matrix = []\n",
    "        self.similarity_matrix_soft = []\n",
    "        self.flags = flags\n",
    "\n",
    "    def get_spans(self,pattern):\n",
    "        \"Takes a pattern and locates the spans of the matches.\"\n",
    "        if not pattern in self.pattern2chars_matched:\n",
    "            spans = list(enumerate([result.span() for result in re.finditer(pattern,self.string,flags=self.flags)]))\n",
    "            print('------ Pattern: %s\\t Matched %d patterns -----' %(pattern,len(spans)))\n",
    "            self.pattern2span.append((pattern,spans))\n",
    "            match_n = 0\n",
    "            for num,span in spans:\n",
    "                match_n+=span[1]-span[0]\n",
    "            self.pattern2chars_matched[pattern] = match_n\n",
    "            self.pattern2idx[pattern] = len(self.pattern2chars_matched) -1\n",
    "            self.pattern2n_match[pattern] = len(spans)\n",
    "            self.patterns.append(pattern)\n",
    "        else:\n",
    "            print('------ Pattern: %s\\t Matched %d patterns -----' %(pattern,self.pattern2n_match[pattern]))\n",
    "\n",
    "\n",
    "    def has_overlap(span,span2):\n",
    "        \"Locates overlap between two pattern spans\"\n",
    "        if span==span2:\n",
    "            return True\n",
    "        for val in span:\n",
    "            if span2[0]<=val<=span2[1]:\n",
    "                return True\n",
    "        return False\n",
    "    def make_overlap_network(self):\n",
    "        \"Constructs Networks between patterns and spans, span2span and pattern2pattern.\"\n",
    "        patterns = self.pattern2span\n",
    "        done = self.pattern_comparisons\n",
    "        span_g = self.span2span\n",
    "        pattern2pattern = self.pattern2pattern\n",
    "        pattern2pattern_soft = self.pattern2pattern_soft\n",
    "        for i in range(len(patterns)-1):\n",
    "            pattern,spans = patterns[i]\n",
    "            for j in range(i+1,len(patterns)):\n",
    "                if (i,j) in done:\n",
    "                    continue\n",
    "                pattern2,spans2 = patterns[j]\n",
    "                pattern_pair = (i,j)\n",
    "                for num,span in spans:\n",
    "                    size = span[1]-span[0]\n",
    "                    for num2,span2 in spans2:\n",
    "                        size2 = span2[1]-span2[0]\n",
    "                        overlap = get_span_overlap(span,span2)\n",
    "                        if overlap<=0:\n",
    "                            continue\n",
    "\n",
    "                        span_sum = size2+size - overlap\n",
    "                        sim = overlap/span_sum\n",
    "                        n,n2 = '%d_%d'%(i,num),'%d_%d'%(j,num2)\n",
    "                        span_g.add_edge(n,n2)\n",
    "                        span_g[n][n2]['similarity'] = sim\n",
    "                        span_g.nodes[n]['pattern'] = i\n",
    "                        span_g.nodes[n2]['pattern'] = j\n",
    "                        if sim==1:\n",
    "                            try:\n",
    "                                pattern2pattern[pattern_pair].add(n)\n",
    "                                pattern2pattern[pattern_pair].add(n2)\n",
    "                            except:\n",
    "                                pattern2pattern[pattern_pair] = set()\n",
    "                                pattern2pattern[pattern_pair].add(n)\n",
    "                                pattern2pattern[pattern_pair].add(n2)\n",
    "                        try:\n",
    "                            pattern2pattern_soft[pattern_pair].add(n)\n",
    "                            pattern2pattern_soft[pattern_pair].add(n2)\n",
    "                        except:\n",
    "                            pattern2pattern_soft[pattern_pair] = set()\n",
    "                            pattern2pattern_soft[pattern_pair].add(n)\n",
    "                            pattern2pattern_soft[pattern_pair].add(n2)\n",
    "\n",
    "                done.add((i,j))\n",
    "    def explore_pattern(self,pattern,n_samples=10,context=10,shuffle=True):\n",
    "        \"\"\"Prints examples of matches including context. Use the context argument for in- or decreasing the context.\n",
    "        \"\"\"\n",
    "        self.get_spans(pattern)\n",
    "        idx = self.pattern2idx[pattern]\n",
    "        spans = [i[1] for i in self.pattern2span[idx][1]]\n",
    "        n_samples = min([n_samples,len(spans)])\n",
    "        if shuffle:\n",
    "            sample = random.sample(spans,n_samples)\n",
    "        else:\n",
    "            sample = spans[0:n_samples]\n",
    "        for start,stop in sample:\n",
    "            match = self.string[start:stop]\n",
    "            start,stop = max([start-context,0]),min([stop+context,len(self.string)])\n",
    "            context_string = self.string[start:stop]\n",
    "            print('Match: %s\\tContext:%s'%(match,context_string))\n",
    "\n",
    "    def explore_difference(self,pattern,pattern2,method='soft',context = 0):\n",
    "        \"\"\"returns two lists of matches only matched by one of the expressions and not in the other.\n",
    "        Match can be defined as either a perfect match (hard) or overlap between matches (soft).\n",
    "        Input:\n",
    "            pattern: regular expression string\n",
    "            pattern2: regular expression string\n",
    "            context: defines how much context of the non matches you will see\n",
    "            method : define the matching method [hard, soft]\n",
    "        Return:\n",
    "            list of pattern1 matches not matched by pattern2,list of pattern2 matches not matched by pattern1\n",
    "        \"\"\"\n",
    "\n",
    "        # check if patterns have been matched.\n",
    "        self.get_spans(pattern)\n",
    "        self.get_spans(pattern2)\n",
    "        # add the spans to the overlap network.\n",
    "        self.make_overlap_network()\n",
    "        diff = []\n",
    "        pat_idx,pat_idx2 = self.pattern2idx[pattern],self.pattern2idx[pattern2]\n",
    "        pattern_pair = tuple(sorted([pat_idx,pat_idx2]))\n",
    "\n",
    "        if method=='soft':\n",
    "            if pattern_pair in self.pattern2pattern_soft:\n",
    "                overlap = self.pattern2pattern_soft[pattern_pair]\n",
    "            else:\n",
    "                overlap = set()\n",
    "        elif method=='hard':\n",
    "            if pattern_pair in self.pattern2pattern:\n",
    "                overlap = self.pattern2pattern[pattern_pair]\n",
    "            else:\n",
    "                overlap = set()\n",
    "        else:\n",
    "            print('Error: you need to define the method as either soft or hard')\n",
    "            return\n",
    "        for (num,span) in self.pattern2span[pat_idx][1]:\n",
    "            n = '%d_%d'%(pat_idx,num)\n",
    "            if not n in overlap:\n",
    "                diff.append(self.string[max([span[0]-context,0]):min([span[1]+context,len(self.string)])])\n",
    "        diff2 = []\n",
    "        for num,span in self.pattern2span[pat_idx2][1]:\n",
    "            n = '%d_%d'%(pat_idx2,num)\n",
    "            if not n in overlap:\n",
    "                diff2.append(self.string[max([span[0]-context,0]):min([span[1]+context,len(self.string)])])\n",
    "        print('''Found %d overlaps between the expressions:\n",
    "        pattern1: %s \\t and\n",
    "        pattern2: %s\n",
    "        %d included in pattern1 and not in the pattern2\n",
    "        %d was included in pattern2 and not in pattern1'''%(len(overlap),pattern,pattern2,len(diff),len(diff2)))\n",
    "        return diff,diff2\n",
    "    def update_spans(self):\n",
    "        \"Updates matches if a new string is defined.\"\n",
    "        self.pattern2span = []\n",
    "        patterns = list(self.pattern2chars_matched)\n",
    "        self.pattern2chars_matched = {}\n",
    "        for pattern in self.patterns:\n",
    "            self.get_spans(pattern)\n",
    "    def define_string_sample(self,string):\n",
    "        \"Defines and updates the string to explore matches with.\"\n",
    "        self.string = string\n",
    "        self.update_spans()\n",
    "    def create_similarity_matrix(self,method='hard'):\n",
    "        \"Creates a directed similarity matrix between patterns defined.\"\n",
    "        self.make_overlap_network()\n",
    "        pat2n = self.pattern2n_match\n",
    "        patterns = [i[0] for i in self.pattern2span]\n",
    "        #if len(self.similarity_matrix) == len(patterns): # check if it is already defined.\n",
    "         #   return None\n",
    "        if method =='soft':\n",
    "            g = self.pattern2pattern_soft\n",
    "            if len(self.similarity_matrix_soft)==len(self.patterns):\n",
    "                return\n",
    "        else:\n",
    "            if len(self.similarity_matrix)==len(self.patterns):\n",
    "                return\n",
    "            g = self.pattern2pattern\n",
    "        mat = np.empty((len(patterns),len(patterns)))\n",
    "        mat[:] = np.nan\n",
    "        for i in range(len(patterns)-1):\n",
    "            n = self.pattern2n_match[patterns[i]]\n",
    "\n",
    "            for j in range(i+1,len(patterns)):\n",
    "                n2 = self.pattern2n_match[patterns[j]]\n",
    "                pattern_pair = (i,j)\n",
    "                try:\n",
    "                    overlap = len(g[pattern_pair])/2\n",
    "                except:\n",
    "                    overlap = 0\n",
    "                #sum_ = n+n2 - overlap\n",
    "                #try:\n",
    "                #    sim = overlap/sum_\n",
    "                #except:\n",
    "                #    sim = np.nan\n",
    "                if n>0:\n",
    "                    sim = overlap/n\n",
    "                else:\n",
    "                    sim = np.nan\n",
    "                mat[i][j] = sim\n",
    "                if n2>0:\n",
    "                    sim = overlap/n2\n",
    "                else:\n",
    "                    sim = np.nan\n",
    "                mat[j][i] = sim\n",
    "        if method=='soft':\n",
    "            self.similarity_matrix_soft = mat\n",
    "        if method=='hard':\n",
    "            self.similarity_matrix = mat\n",
    "    def plot_similarity(self,method='hard'):\n",
    "        \"\"\"Plots a directed similarity matrix between patterns.\n",
    "        The similarity is defined as number of overlapping matches divided by number of matches.\n",
    "        The definition of overlapping matches between two patterns can be changed from hard (only exact matches) to soft (matches has overlap),\n",
    "        This will allow you to investigate two different things:\n",
    "            * Using the 'hard' method you can see how patterns\n",
    "            * Using 'soft' you can see how expressions narrows the number of accepted patterns.\n",
    "        method: str ['hard','soft'] parameter for defining overlap between regular expression matches. 'hard' entails exact match, and 'soft' defines match as an overlap between matches.\n",
    "         \"\"\"\n",
    "        patterns = self.patterns\n",
    "        self.create_similarity_matrix(method)\n",
    "        if method=='soft':\n",
    "            mat = self.similarity_matrix_soft\n",
    "        else:\n",
    "            mat = self.similarity_matrix\n",
    "        plt.figure(figsize=(12,8))\n",
    "        sns.heatmap(mat,cmap='viridis')\n",
    "        plt.xticks(np.arange(len(patterns))+.5,patterns,rotation=45)\n",
    "        plt.yticks(np.arange(len(patterns))+.5,patterns,rotation=0)\n",
    "        plt.title('Similarity Matrix')\n",
    "    def report(self,method='hard',plot=True):\n",
    "        \"\"\"Report the number of matches of each pattern developed and plot a similarity matrix between them.\n",
    "        The similarity is defined as number of overlapping matches divided by number of matches.\n",
    "        The definition of overlapping matches between two patterns can be changed from hard (only exact matches) to soft (matches has overlap),\n",
    "        This will allow you to investigate two different things:\n",
    "            * Using the 'hard' method you can see how patterns\n",
    "            * Using 'soft' you can see how expressions narrows the number of accepted patterns.\n",
    "        method: str ['hard','soft'] parameter for defining overlap between regular expression matches. 'hard' entails exact match, and 'soft' defines match as an overlap between matches.\n",
    "        \"\"\"\n",
    "        for pattern,n in self.pattern2n_match.items():\n",
    "            print('------ Pattern: %s\\t Matched %d patterns -----' %(pattern,n))\n",
    "        if plot:\n",
    "            self.plot_similarity(method)\n",
    "    def compile_pattern(self,pattern):\n",
    "        \"\"\"Method to compile the final pattern using the default flags set\"\"\"\n",
    "        return re.compile(pattern,flags=self.flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Pattern: kr\t Matched 0 patterns -----\n",
      "------ Pattern: kr\t Matched 0 patterns -----\n",
      "Found 0 overlaps between the expressions:\n",
      "        pattern1: kr \t and\n",
      "        pattern2: kr\n",
      "        0 included in pattern1 and not in the pattern2\n",
      "        0 was included in pattern2 and not in pattern1\n",
      "------ Pattern: [0-9]+kr\t Matched 0 patterns -----\n",
      "------ Pattern: kr\t Matched 0 patterns -----\n",
      "Found 0 overlaps between the expressions:\n",
      "        pattern1: [0-9]+kr \t and\n",
      "        pattern2: kr\n",
      "        0 included in pattern1 and not in the pattern2\n",
      "        0 was included in pattern2 and not in pattern1\n",
      "------ Pattern: [0-9]+(?:[,.][0-9]+)?kr\t Matched 0 patterns -----\n",
      "------ Pattern: kr\t Matched 0 patterns -----\n",
      "Found 0 overlaps between the expressions:\n",
      "        pattern1: [0-9]+(?:[,.][0-9]+)?kr \t and\n",
      "        pattern2: kr\n",
      "        0 included in pattern1 and not in the pattern2\n",
      "        0 was included in pattern2 and not in pattern1\n",
      "------ Pattern: [0-9]+(?:[,.][0-9]+)?\\s{0,2}kr\t Matched 0 patterns -----\n",
      "------ Pattern: kr\t Matched 0 patterns -----\n",
      "Found 0 overlaps between the expressions:\n",
      "        pattern1: [0-9]+(?:[,.][0-9]+)?\\s{0,2}kr \t and\n",
      "        pattern2: kr\n",
      "        0 included in pattern1 and not in the pattern2\n",
      "        0 was included in pattern2 and not in pattern1\n",
      "------ Pattern: [0-9]+(?:[,.][0-9]+)?\\s{0,5}kr(?:oner)?\t Matched 0 patterns -----\n",
      "------ Pattern: kr\t Matched 0 patterns -----\n",
      "Found 0 overlaps between the expressions:\n",
      "        pattern1: [0-9]+(?:[,.][0-9]+)?\\s{0,5}kr(?:oner)? \t and\n",
      "        pattern2: kr\n",
      "        0 included in pattern1 and not in the pattern2\n",
      "        0 was included in pattern2 and not in pattern1\n",
      "------ Pattern: [0-9]+kr\t Matched 0 patterns -----\n"
     ]
    }
   ],
   "source": [
    "explore_money = ExploreRegex(sample_string)\n",
    "\n",
    "first = 'kr'\n",
    "second = '[0-9]+kr'\n",
    "third = '[0-9]+(?:[,.][0-9]+)?kr'\n",
    "fourth = '[0-9]+(?:[,.][0-9]+)?\\s{0,2}kr'\n",
    "final = '[0-9]+(?:[,.][0-9]+)?\\s{0,5}kr(?:oner)?'\n",
    "\n",
    "patterns = [first,second,third,fourth,final]\n",
    "\n",
    "for pattern in patterns:\n",
    "    explore_money.explore_difference(pattern,patterns[0])\n",
    "\n",
    "explore_money.explore_pattern(second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API > Scraping\n",
    "- Use the API if provided\n",
    "- Create a developer account\n",
    "- Learn how to Authenticate by reading their documentation\n",
    "- Construct your queries and collec the data\n",
    "\n",
    "**Examples**\n",
    "- Twitter, YouTube, Reddit, Facebook, Github, Stackexchange, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting something into production?\n",
    "Right now you are working locally on your computer. That is fine for small project, but some might become data scientist working in organizations that would like to put your work into production. Then we need to be really good at version control (git) but also working with servers and databases. This is obviously not part of this course but you might still want to check [RunDeck ](https://www.rundeck.com/open-source) out. It is a open-source tool you can use to orchestra your scripts. Let's say you want your scraper to run every day at 08:00 am, then you can schedule that job in RunDeck or similar tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Junior Data Scientist Position\n",
    "A friend of mine, who is a business owner, is looking to hire a junior data scientist. Sent me two or three of the following documents and you might end up getting a kewl job.\n",
    "1. Code that collects Popular Times data from [this url](https://www.google.com/search?sxsrf=ALeKk02OxWY1MFf29v-44s8Bnozf6EJyHA%3A1597134428470&ei=XFYyX6OTHK_qrgS714u4Ag&q=torvehallerne&oq=torvehallerne&gs_lcp=CgZwc3ktYWIQAzIOCC4QxwEQrwEQywEQkwIyCwguEMcBEK8BEMsBMgsILhDHARCvARDLATILCC4QxwEQrwEQywEyCwguEMcBEK8BEMsBMgsILhDHARCvARDLATIFCAAQywEyCwguEMcBEK8BEMsBMgsILhDHARCvARDLATIFCAAQywE6BwguECcQkwI6BAgAEEM6BwgAEBQQhwI6AggAOg0ILhDHARCvARAnEJMCOgQIIxAnOgUILhCRAjoFCAAQkQI6CAguEMcBEKMCOgoILhDHARCvARBDOgIILjoICC4QxwEQrwE6CwguEMcBEK8BEJMCUJKQAVismgFglZsBaABwAHgBgAGuAYgB7AySAQM0LjmYAQCgAQGqAQdnd3Mtd2l6wAEB&sclient=psy-ab&ved=0ahUKEwijsZGy3ZLrAhUvtYsKHbvrAicQ4dUDCAw&uact=5). Stores and other public places has something called Popular Times when you google them. It indicates how many peoples geolokation are at the place in diffrent time interval. collects data from [this url](https://www.linkedin.com/in/nicklasjohansen/). \n",
    "2. Code that collects LinkedIn data from this url together with a simple idea of how you would use LinkedIn data for a social data science project.\n",
    "3. your_resume.pdf\n",
    "\n",
    "Reach me by nj@sodas.ku.dk. Deadline is september 1st.\n",
    "\n",
    "\n",
    "# Master's Thesis Collaboration\n",
    "I am funded by the [HOPE Project](https://politicalscience.ku.dk/research/projects/hope/) investigating **H**ow Democracies C**ope** with Covid19. We offer thesis collaboration until summer 2023 for student who consider a career in academia. Reach out if you have an idea for your thesis that are within the scope of HOPE.  \n",
    "\n",
    "Reach me by nj@sodas.ku.dk to start a dialogue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "# Exercise Set 8: Web Scraping 3\n",
    "\n",
    "In this Exercise Set we shall develop our webscraping skills even further by practicing using `Selenium` while   parsing and navigating html trees using `BeautifoulSoup`. Furthermore we will train extracting information from raw text with no html tags to help, using regex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 8.1: Translating domains into companies\n",
    "This exercise is about solving a problem that danish companies are facing. They all want to use external data such as customer review data to gain more knowledge about their customers and maybe even use the information as features in their models. There is just one problem: users often create reviews for domains (brand name) and not companies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 8.1.1:** You work for the danish authorities and are currently staffed to a project where you have to reduce the amount of dangerous toys. You have build a webscraper that collect user reviews form Trustpilot and have identified some websites that got a bad reputation among its users. You belive that the risk of them selling illegal or dangerous toys might be bigger than some of the bg brands with good ratings and decide to investigate them. \n",
    "\n",
    "> Go to the website https://www.dk-hostmaster.dk/da/find-domaenenavn with selenium and search for \"netbaby.dk\". Store the name of the registrant \"Euphemia Media\" in the variable `company`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 92.0.4515\n",
      "Get LATEST driver version for 92.0.4515\n",
      "Driver [C:\\Users\\danib\\.wdm\\drivers\\chromedriver\\win32\\92.0.4515.107\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euphemia Media\n"
     ]
    }
   ],
   "source": [
    "#[Answer 8.1.1]\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "url = 'https://www.dk-hostmaster.dk/da/find-domaenenavn'\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(url)\n",
    "\n",
    "element = driver.find_element_by_id(\"query_domain\")#//*[@id=\"query_domain\"])\n",
    "element.send_keys(\"netbaby.dk\", Keys.ENTER)\n",
    "name = driver.find_element_by_id(\"domain_registrant_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company = name.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 8.1.2:** Now you know who owns the domain and would like to know more about the company `euphemia media`. \n",
    "\n",
    "> Go to the Central Business Register website https://datacvr.virk.dk/data/. Figure out how to look up companies by changing the url and then lookup `euphemia media`. Store the CVR number in the variable `cvr` and print it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \" \" in \n",
    "\n",
    "base = f\"https://datacvr.virk.dk/data/visninger?soeg={Euphemia%20Media}&oprettet=null&ophoert=null&branche=&type=Alle&sortering=default&language=da\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 8.1.3:** Congratulations. You are now able to translate domains into companies and by that enrich what ever analysis you want to make. Let's say that you were to build a scraper who could translate thousands of domains. What kind of errors can you imagine running into and how would you mitigate them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 8.2: Practicing Regular Expressions.\n",
    "This exercise is about developing your experience with designing your own regular expressions. This is especially relevant for those of you who are going to work with text data in your exams.\n",
    "\n",
    "Remember you can always consult the regular expression reference page [here](https://www.regular-expressions.info/refquick.html), if you need to remember or understand a specific symbol. \n",
    "\n",
    "We will use a sample of the trustpilot dataset that you practiced collecting in module_7.\n",
    "You can load it directly into python from the following link: https://raw.githubusercontent.com/snorreralund/scraping_seminar/master/english_review_sample.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 8.2.1:** Load the data used in the exercise using the `pd.read_csv` function. (Hint: path to file can be both a url or systempath). \n",
    "\n",
    ">Define a variable `sample_string = '\\n'.join(df.sample(2000).reviewBody)` as sample of all the reviews that you will practice on.  (Run it once in a while to get a new sample for potential differences).\n",
    "Imagine we were a company wanting to find the reviews where customers are concerned with the price of a service. They decide to write a regular expression to match all reviews where a currencies and an amount is mentioned. \n",
    "\n",
    "> **Ex. 8.2.2:** \n",
    "> Write an expression that matches both the dollar-sign (\\$) and dollar written literally, and the amount before or after a dollar-sign. Remember that the \"$\"-sign is a special character in regular expressions. Explore and refine using the explore_pattern function in the package I created called explore_regex. \n",
    "```python\n",
    "import explore_regex as e_re\n",
    "explore_regex = e_re.Explore_Regex(sample_string) # Initaizlie the Explore regex Class.\n",
    "explore_regex.explore_pattern(pattern) # Use the .explore_pattern method.\n",
    "```\n",
    "\n",
    "\n",
    "Start with exploring the context around digits (\"\\d\") in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex.8.2.3** Use the .report() method. e_re.report(), and print the all patterns in the development process using the .pattern method - i.e. e_re.patterns \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 8.2.4** \n",
    "Finally write a function that takes in a string and outputs if there is a match. Use the .match function to see if there is a match (hint if does not return a NoneType object - `re.match(pattern,string)!=None`).\n",
    "\n",
    "> Define a column 'mention_currency' in the dataframe, by applying the above function to the text column of the dataframe. \n",
    "*** You should have approximately 310 reviews that matches. - but less is also alright***\n",
    "\n",
    "> **Ex. 8.2.5** Explore the relation between reviews mentioning prices and the average rating. \n",
    "\n",
    "> **Ex. 8.2.6 (extra)** Define a function that outputs the amount mentioned in the review (if more than one the largest), define a new column by applying it to the data, and explore whether reviews mentioning higher prices are worse than others by plotting the amount versus the rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 8.2.7:** Now we write a regular expression to extract emoticons from text.\n",
    "Start by locating all mouths ')' of emoticons, and develop the variations from there. Remember that paranthesis are special characters in regex, so you should use the escape character."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "nav_menu": {
    "height": "328px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
