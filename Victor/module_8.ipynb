{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **DO YOU USE GITHUB?**  \n",
    "If True: print('Remember to make your edits in a personal copy of this notebook')  \n",
    "Else: print('You don't have to understand. Continue your life.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8: Web Scraping 3\n",
    "\n",
    "Now you know how to investigate, download and parse data from a website. Thank you module_6 and module_7. Though, you might still run into websites you are having a difficult time scraping. Getting thorugh such cahllenge using automated browsing will be the main topic of this module. \n",
    "\n",
    "We will continue to learn new techniques of parsing unstructured text and HTML. This will help you build ***custom datasets*** within just a few hours or days work, that would have taken ***months*** to curate and clean manually. We will learn how to automate browsing and use regex to parse text without html tags.\n",
    "\n",
    "\n",
    "Readings for `module 6+7+8`:\n",
    "- [Python for Data Analysis, chapter 6](https://bedford-computing.co.uk/learning/wp-content/uploads/2015/10/Python-for-Data-Analysis.pdf)\n",
    "- [A Practical Introduction to Web Scraping in Python](https://realpython.com/python-web-scraping-practical-introduction/)\n",
    "- [An introduction to web scraping with Python](https://towardsdatascience.com/an-introduction-to-web-scraping-with-python-a2601e8619e5)\n",
    "- [Introduction to Web Scraping using Selenium](https://medium.com/the-andela-way/introduction-to-web-scraping-using-selenium-7ec377a8cf72)\n",
    "\n",
    "Video materiale from `ISDS 2020`:\n",
    "- [Web Scraping 1](https://bit.ly/ISDS2021_6)\n",
    "- [Web Scraping 2](https://bit.ly/ISDS2021_7)\n",
    "- [Web Scraping 3](https://bit.ly/ISDS2021_8)\n",
    "\n",
    "Other ressources:\n",
    "- [Nicklas Webpage](https://nicklasjohansen.netlify.app/)\n",
    "- [Data Driven Organizational Analysis, Fall 2021](https://efteruddannelse.kurser.ku.dk/course/2021-2022/ASTK18379U)\n",
    "- [Master of Science (MSc) in Social Data Science](https://www.socialdatascience.dk/education)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions and Automated Browsing\n",
    "Sometimes scraping tasks demand interactions (e.g. login, scrolling, clicking), and a no XHR data can be found easily, so you need the browser to execute the scripts before you can get the data. XHR is short for XMLHttpRequest - a JavaScript API - like the one we found in the jobnet.dk exerise.\n",
    "\n",
    "Here we use the `Selenium` package in combination with the `ChromeDriver` - you can download the latest release [here](https://chromedriver.chromium.org/downloads). It allows you to animate a browser. \n",
    "\n",
    "Make sure to download the driver as well as the newest version of Selenium. \"pip install selenium\" should do the trick. \n",
    "\n",
    "Some developers prefer to you [geckodriver](https://github.com/mozilla/geckodriver/releases) as an alternative to `ChromeDriver`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 92.0.4515\n",
      "Get LATEST driver version for 92.0.4515\n",
      "There is no [win32] chromedriver for browser 92.0.4515 in cache\n",
      "Get LATEST driver version for 92.0.4515\n",
      "Trying to download new driver from https://chromedriver.storage.googleapis.com/92.0.4515.107/chromedriver_win32.zip\n",
      "Driver has been saved in cache [C:\\Users\\vtodd\\.wdm\\drivers\\chromedriver\\win32\\92.0.4515.107]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "url = 'https:google.com'\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also download the driver to your computer\n",
    "# Save it in your working directory and write the code\n",
    "\n",
    "#import os\n",
    "#directory = os.getcwd()\n",
    "#path = os.path.join(directory, 'chromedriver')\n",
    "#driver = webdriver.Chrome(executable_path=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benifits from autoamting browsing\n",
    "1. You can access data that are not directly in the HTML code but that is being generating while browsing\n",
    "2. You can get thorugh login screens and other scraping barriers\n",
    "3. You can automate browsing behaviour such as scrolling down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: nboards.dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 92.0.4515\n",
      "Get LATEST driver version for 92.0.4515\n",
      "Driver [C:\\Users\\vtodd\\.wdm\\drivers\\chromedriver\\win32\\92.0.4515.107\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "# step 1: load the webpage we want to scrape in our virtual browser\n",
    "url = 'https://nboard.dk/search'\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 92.0.4515\n",
      "Get LATEST driver version for 92.0.4515\n",
      "Driver [C:\\Users\\vtodd\\.wdm\\drivers\\chromedriver\\win32\\92.0.4515.107\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "# step 2: scroll down the page to load more profiles\n",
    "import time\n",
    "\n",
    "url = 'https://nboard.dk/search'\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "for i in range(5):\n",
    "    time.sleep(3)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 92.0.4515\n",
      "Get LATEST driver version for 92.0.4515\n",
      "Driver [C:\\Users\\vtodd\\.wdm\\drivers\\chromedriver\\win32\\92.0.4515.107\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 23.51 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# step 3: save the soup and keep track of runtime\n",
    "\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "url = 'https://nboard.dk/search'\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(url)\n",
    "\n",
    "for i in range(5):\n",
    "    time.sleep(3)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "print(\"--- %s seconds ---\" % round((time.time() - start_time),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Cache is valid for [12/08/2020]\n",
      "[WDM] - Looking for [chromedriver 84.0.4147.30 mac64] driver in cache \n",
      "[WDM] - Driver found in cache [/Users/nicklasjohansen/.wdm/drivers/chromedriver/84.0.4147.30/mac64/chromedriver]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--- 193.0 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# step 3: save the soup and keep track of runtime\n",
    "\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "url = 'https://nboard.dk/search'\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "lenOfPage = driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "match=False\n",
    "while(match==False):\n",
    "    lastCount = lenOfPage\n",
    "    time.sleep(1)\n",
    "    lenOfPage = driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "    if lastCount==lenOfPage:\n",
    "        match=True\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "print(\"--- %s seconds ---\" % round((time.time() - start_time),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n",
      "https://nboard.dk/candidate_profile/Leif-Vestergaard\n"
     ]
    }
   ],
   "source": [
    "# step 4: use the soup to generate our mapping of urls (profiles) that we want to scrape\n",
    "\n",
    "names = soup.find_all('span', {'class': 'name'})\n",
    "\n",
    "urls = []\n",
    "for i in range(len(names)):\n",
    "    temp = 'https://nboard.dk/candidate_profile/'+ str(names[i].text)\n",
    "    temp = temp.replace(' ','-')\n",
    "    temp = temp.replace('--','-')\n",
    "    urls.append(temp)\n",
    "\n",
    "print(len(urls))\n",
    "print(urls[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 30.18 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>location</th>\n",
       "      <th>resume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Leif Vestergaard</td>\n",
       "      <td>Vækst fokuseret bestyrelsesformand/bestyrelses...</td>\n",
       "      <td>København, Danmark</td>\n",
       "      <td>I mine +15 år som bestyrelsesmedlem har jeg ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Claus Andersen-Aagaard</td>\n",
       "      <td>Best in class økonomidirektør med bred teknisk...</td>\n",
       "      <td>Sermersooq, Grønland</td>\n",
       "      <td>Som Økonomidirektør og tidligere konstitueret ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mette Thorgaard</td>\n",
       "      <td>20+ års international c-level erfaring</td>\n",
       "      <td>København, Danmark</td>\n",
       "      <td>Brænder efter at hjælpe den gode ide til at vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Titti Kopp</td>\n",
       "      <td>Advokat - erfaring med store bygge- og anlægsp...</td>\n",
       "      <td>København, Danmark</td>\n",
       "      <td>Jeg er advokat og ph.d. med 20 års erfaring i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Inooraq Brandt</td>\n",
       "      <td>Ledelses- og bestyrelseserfaring med strategis...</td>\n",
       "      <td>Sermersooq, Grønland</td>\n",
       "      <td>Mine styrker inkluderer en yderst analytisk ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Christina Andersen</td>\n",
       "      <td>Erfaren leder i Life Science/Pharma</td>\n",
       "      <td>Ballerup, Danmark</td>\n",
       "      <td>Strategi- og forretningsudvikling\\nTransformat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jan Høstrup</td>\n",
       "      <td>Erfaren organisationskonsulent</td>\n",
       "      <td>Aarhus, Danmark</td>\n",
       "      <td>Med en historie som intern og ekstern konsulen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mikkel Vejlgaard</td>\n",
       "      <td>Erfaren digital leder, der elsker vækst.</td>\n",
       "      <td>København, Danmark</td>\n",
       "      <td>Jeg er en kommerciel profil med mere end 20 år...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name                                           subtitle  \\\n",
       "0        Leif Vestergaard  Vækst fokuseret bestyrelsesformand/bestyrelses...   \n",
       "1  Claus Andersen-Aagaard  Best in class økonomidirektør med bred teknisk...   \n",
       "2         Mette Thorgaard             20+ års international c-level erfaring   \n",
       "3              Titti Kopp  Advokat - erfaring med store bygge- og anlægsp...   \n",
       "4          Inooraq Brandt  Ledelses- og bestyrelseserfaring med strategis...   \n",
       "5      Christina Andersen                Erfaren leder i Life Science/Pharma   \n",
       "6             Jan Høstrup                     Erfaren organisationskonsulent   \n",
       "7        Mikkel Vejlgaard           Erfaren digital leder, der elsker vækst.   \n",
       "\n",
       "               location                                             resume  \n",
       "0    København, Danmark  I mine +15 år som bestyrelsesmedlem har jeg ha...  \n",
       "1  Sermersooq, Grønland  Som Økonomidirektør og tidligere konstitueret ...  \n",
       "2    København, Danmark  Brænder efter at hjælpe den gode ide til at vo...  \n",
       "3    København, Danmark  Jeg er advokat og ph.d. med 20 års erfaring i ...  \n",
       "4  Sermersooq, Grønland  Mine styrker inkluderer en yderst analytisk ti...  \n",
       "5     Ballerup, Danmark  Strategi- og forretningsudvikling\\nTransformat...  \n",
       "6       Aarhus, Danmark  Med en historie som intern og ekstern konsulen...  \n",
       "7    København, Danmark  Jeg er en kommerciel profil med mere end 20 år...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 5: scraping profiles \n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "name = []\n",
    "subtitle = []\n",
    "location = []\n",
    "resume = []\n",
    "\n",
    "for i in range(10): #len(urls)\n",
    "    response = requests.get(urls[i])\n",
    "    html = response.text\n",
    "    \n",
    "    if 'Internal server error' in html:\n",
    "        continue\n",
    "    \n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    name.append(soup.find('title').text)\n",
    "    subtitle.append(soup.find('span', {'class': 'sub-title'}).text)\n",
    "    location.append(soup.find('span', {'class': 'location'}).text)\n",
    "    resume.append(soup.find('span', {'class': 'resume'}).text)\n",
    "\n",
    "df = pd.DataFrame({'name':name, \n",
    "                   'subtitle':subtitle, \n",
    "                   'location':location, \n",
    "                   'resume':resume})\n",
    "\n",
    "print(\"--- %s seconds ---\" % round((time.time() - start_time),2))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next level scrapers\n",
    "\n",
    "You have know learned some of the fundamentals of collecting and parsing data and should be well suited for your exam project. Though I find it important to adress that you might run into some challenges that we have not learn dealing with yet. Facebook, LinkedIn, Google and all the other big tech firms are battling scrapers and has done all kinds of thing to make it hard for us to steal public data on their sites. I have found som article that you might find interessting.\n",
    "\n",
    "- [Most Commonly used techniques to Prevent Scraping:](https://medium.com/@betoayesa/using-the-content-as-an-anti-scrape-weapon-draft-9bb10cd30e5c)\n",
    "- [Advanced Web Scraping Tactics](https://www.pluralsight.com/guides/advanced-web-scraping-tactics-python-playbook)\n",
    "- [Scraping Sites That Use JavaScript and AJAX](https://oup-arc.com/protected/files/content/file/1505319833942-CH9---Scraping-Sites-that-Use-JavaScript-and-AJAX.pdf)\n",
    "- [Get Started Scraping LinkedIn With Python and Selenium](https://medium.com/nerd-for-tech/linked-in-web-scraper-using-selenium-15189959b3ba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting patterns from Raw Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Standard string operations**\n",
    "- `string.split`\n",
    "- `string.strip`\n",
    "- `string.replace`\n",
    "\n",
    "**Regex**\n",
    "\"A regular expression (shortened as regex) is a sequence of characters that define a search pattern. Usually such patterns are used by string-searching algorithms for \"find\" or \"find and replace\" operations on strings, or for input validation.\n",
    "\n",
    "**Examples**\n",
    "- extract currency and amount from raw text: $ 20, 10.000 dollars 10,000 £\n",
    "- email addresses: here you want to design a pattern (as above), that captures only the uses of @ within an email.\n",
    "- urls. Here you are trying to define all the different ways of writing urls (https, http, no http). \n",
    "- Dates. Again many variations: 17th of June 2017, 06/17/17 or 17. June 17\n",
    "- addresses, \n",
    "- phone numbers: 8888888 or 88 88 88 88 or +45 88 88 88 88,\n",
    "- emojiies in text. Capturing all the different ways of expressing smiley faces with one regular expression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "line = \"should we use regex more often? let me know at nj@sodas.ku.dk\"\n",
    "match = re.search(r'[\\w\\.-]+@[\\w\\.-]+', line)\n",
    "match.group(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ressources**\n",
    "- Community and interactive playground [here](http://regexr.com/)\n",
    "- Interactive tutorial [here](https://regexone.com/)\n",
    "- Lookup all special characters [here](https://www.regular-expressions.info/refquick.html)\n",
    "\n",
    "**Syntax for build your own expresions**\n",
    "* \\+ = 1 or more times  -- e.g. \"a+\" will match: \"a\", and \"aaa\"\n",
    "* \\* = 0 or more times  -- e.g. \"ba*\" will match: \"b\", and \"ba\", and \"baaa\"\n",
    "* {3} = exactly three times --- e.g. \"ba{3}\" will match \"baaa\", but not \"baa\"\n",
    "* ? = once or none\n",
    "* \\\\ = escape character, used to find characters that has special meaning with regex: e.g. \\+ \\*\n",
    "* [] = allows you to define a set of characters\n",
    "* ^ = applied within a set, it becomes the inverse of the set defined. Applied outside a set it entails the beginning of a string. $ entails the end of a string.\n",
    "* . = any characters except line break\n",
    "* | = or statement. -- e.g. a|b means find characters a or b.\n",
    "* \\d = digits\n",
    "* \\D = any-non-digits.\n",
    "* \\s = whitespace-separator\n",
    "\n",
    "Sequences\n",
    "* (?:) = Defines a Non-capturing group. -- e.g. \"(?:abc)+\", will match \"abc\" and \"abcabcabc\", but not \"aabbcc\"\n",
    "* (?=)\t= Positive lookahead - only match a certain pattern if a certain pattern comes after it.\n",
    "* (?!)\t= Negative lookahead - only match a certain pattern if **not** a certain pattern comes after it.\n",
    "* (?<=)\t= Positive lookbehind - only match a certain pattern if a certain pattern precedes it.\n",
    "* (?<!) = Negative lookbehind - only match a certain pattern if **not** a certain pattern precedes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For det første findes der en postindleveringssted tættere på hvor jeg bor, pakken skulle have havnet der, ikke i Lyngby.\\n\\nPoststedet i Lyngby var underbemandet i forhold til antallet af folk der vil hente og indlevere pakker. Da jeg trak et nummer kunne jeg se at der var 35 foran mig i køen! Området kunne ikke rumme alle os der stod og ventede. Det tog 38 minutter at hente pakken! Det er alt for dårligt.\\nRejsefebers søgefunktion med +3 dage på hver sin side er fremragende, men udvalget er ikke komplet f.s.v.a. flyafgange. Man er nødsaget til at supplere med andre søgemaskiner for at være helt sikker.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/snorreralund/scraping_seminar/master/danish_review_sample.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "digit_re = re.compile('[0-9]+') # compiled regular expression for matching digits\n",
    "df['hasNumber'] = df.reviewBody.apply(lambda x: len(digit_re.findall(x))>0) # check if it has a number\n",
    "sample_string = '\\n'.join(df[df.hasNumber].sample(2).reviewBody)\n",
    "#sample_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing expressions**  \n",
    "Our SODAS collegaue Snorre has developed his own regex library for python. You can either use it by copying his entire class \"ExploreRegex\" into your notebook or by following this peace of code:\n",
    "```python \n",
    "# download module\n",
    "url = 'https://raw.githubusercontent.com/snorreralund/explore_regex/master/explore_regex.py'\n",
    "response = requests.get(url)\n",
    "# write script to your folder to create a locate module\n",
    "with open('explore_regex.py','w') as f:\n",
    "    f.write(response.text)\n",
    "# import local module\n",
    "import ExploreRegex\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/snorreralund/explore_regex/blob/master/explore_regex.py\n",
    "\n",
    "import networkx as nx\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def get_span_overlap(span,span2):\n",
    "        return min([span[1],span2[1]])-max([span[0],span2[0]])\n",
    "\n",
    "class ExploreRegex():\n",
    "    \"\"\"This module should allow you to compare the differences in matches between regular expressions.\n",
    "    Default Flags are by design RE.UNICODE and re.DOTALL.\n",
    "    \"\"\"\n",
    "    def __init__(self,sample_string,flags=re.DOTALL|re.UNICODE):\n",
    "        self.string = sample_string\n",
    "        self.patterns = []\n",
    "        self.pattern2span = [] # this container stores all the matches of each pattern.\n",
    "        self.span_graph = nx.Graph() # defines network that\n",
    "        self.span2span = nx.Graph()\n",
    "        self.pattern2pattern = {}\n",
    "        self.pattern2pattern_soft = {}\n",
    "        self.pattern2n_match = {}\n",
    "        self.pattern2chars_matched = {}\n",
    "        self.pattern2idx = {}\n",
    "        self.pattern_comparisons = set()\n",
    "        self.similarity_matrix = []\n",
    "        self.similarity_matrix_soft = []\n",
    "        self.flags = flags\n",
    "\n",
    "    def get_spans(self,pattern):\n",
    "        \"Takes a pattern and locates the spans of the matches.\"\n",
    "        if not pattern in self.pattern2chars_matched:\n",
    "            spans = list(enumerate([result.span() for result in re.finditer(pattern,self.string,flags=self.flags)]))\n",
    "            print('------ Pattern: %s\\t Matched %d patterns -----' %(pattern,len(spans)))\n",
    "            self.pattern2span.append((pattern,spans))\n",
    "            match_n = 0\n",
    "            for num,span in spans:\n",
    "                match_n+=span[1]-span[0]\n",
    "            self.pattern2chars_matched[pattern] = match_n\n",
    "            self.pattern2idx[pattern] = len(self.pattern2chars_matched) -1\n",
    "            self.pattern2n_match[pattern] = len(spans)\n",
    "            self.patterns.append(pattern)\n",
    "        else:\n",
    "            print('------ Pattern: %s\\t Matched %d patterns -----' %(pattern,self.pattern2n_match[pattern]))\n",
    "\n",
    "\n",
    "    def has_overlap(span,span2):\n",
    "        \"Locates overlap between two pattern spans\"\n",
    "        if span==span2:\n",
    "            return True\n",
    "        for val in span:\n",
    "            if span2[0]<=val<=span2[1]:\n",
    "                return True\n",
    "        return False\n",
    "    def make_overlap_network(self):\n",
    "        \"Constructs Networks between patterns and spans, span2span and pattern2pattern.\"\n",
    "        patterns = self.pattern2span\n",
    "        done = self.pattern_comparisons\n",
    "        span_g = self.span2span\n",
    "        pattern2pattern = self.pattern2pattern\n",
    "        pattern2pattern_soft = self.pattern2pattern_soft\n",
    "        for i in range(len(patterns)-1):\n",
    "            pattern,spans = patterns[i]\n",
    "            for j in range(i+1,len(patterns)):\n",
    "                if (i,j) in done:\n",
    "                    continue\n",
    "                pattern2,spans2 = patterns[j]\n",
    "                pattern_pair = (i,j)\n",
    "                for num,span in spans:\n",
    "                    size = span[1]-span[0]\n",
    "                    for num2,span2 in spans2:\n",
    "                        size2 = span2[1]-span2[0]\n",
    "                        overlap = get_span_overlap(span,span2)\n",
    "                        if overlap<=0:\n",
    "                            continue\n",
    "\n",
    "                        span_sum = size2+size - overlap\n",
    "                        sim = overlap/span_sum\n",
    "                        n,n2 = '%d_%d'%(i,num),'%d_%d'%(j,num2)\n",
    "                        span_g.add_edge(n,n2)\n",
    "                        span_g[n][n2]['similarity'] = sim\n",
    "                        span_g.nodes[n]['pattern'] = i\n",
    "                        span_g.nodes[n2]['pattern'] = j\n",
    "                        if sim==1:\n",
    "                            try:\n",
    "                                pattern2pattern[pattern_pair].add(n)\n",
    "                                pattern2pattern[pattern_pair].add(n2)\n",
    "                            except:\n",
    "                                pattern2pattern[pattern_pair] = set()\n",
    "                                pattern2pattern[pattern_pair].add(n)\n",
    "                                pattern2pattern[pattern_pair].add(n2)\n",
    "                        try:\n",
    "                            pattern2pattern_soft[pattern_pair].add(n)\n",
    "                            pattern2pattern_soft[pattern_pair].add(n2)\n",
    "                        except:\n",
    "                            pattern2pattern_soft[pattern_pair] = set()\n",
    "                            pattern2pattern_soft[pattern_pair].add(n)\n",
    "                            pattern2pattern_soft[pattern_pair].add(n2)\n",
    "\n",
    "                done.add((i,j))\n",
    "    def explore_pattern(self,pattern,n_samples=10,context=10,shuffle=True):\n",
    "        \"\"\"Prints examples of matches including context. Use the context argument for in- or decreasing the context.\n",
    "        \"\"\"\n",
    "        self.get_spans(pattern)\n",
    "        idx = self.pattern2idx[pattern]\n",
    "        spans = [i[1] for i in self.pattern2span[idx][1]]\n",
    "        n_samples = min([n_samples,len(spans)])\n",
    "        if shuffle:\n",
    "            sample = random.sample(spans,n_samples)\n",
    "        else:\n",
    "            sample = spans[0:n_samples]\n",
    "        for start,stop in sample:\n",
    "            match = self.string[start:stop]\n",
    "            start,stop = max([start-context,0]),min([stop+context,len(self.string)])\n",
    "            context_string = self.string[start:stop]\n",
    "            print('Match: %s\\tContext:%s'%(match,context_string))\n",
    "\n",
    "    def explore_difference(self,pattern,pattern2,method='soft',context = 0):\n",
    "        \"\"\"returns two lists of matches only matched by one of the expressions and not in the other.\n",
    "        Match can be defined as either a perfect match (hard) or overlap between matches (soft).\n",
    "        Input:\n",
    "            pattern: regular expression string\n",
    "            pattern2: regular expression string\n",
    "            context: defines how much context of the non matches you will see\n",
    "            method : define the matching method [hard, soft]\n",
    "        Return:\n",
    "            list of pattern1 matches not matched by pattern2,list of pattern2 matches not matched by pattern1\n",
    "        \"\"\"\n",
    "\n",
    "        # check if patterns have been matched.\n",
    "        self.get_spans(pattern)\n",
    "        self.get_spans(pattern2)\n",
    "        # add the spans to the overlap network.\n",
    "        self.make_overlap_network()\n",
    "        diff = []\n",
    "        pat_idx,pat_idx2 = self.pattern2idx[pattern],self.pattern2idx[pattern2]\n",
    "        pattern_pair = tuple(sorted([pat_idx,pat_idx2]))\n",
    "\n",
    "        if method=='soft':\n",
    "            if pattern_pair in self.pattern2pattern_soft:\n",
    "                overlap = self.pattern2pattern_soft[pattern_pair]\n",
    "            else:\n",
    "                overlap = set()\n",
    "        elif method=='hard':\n",
    "            if pattern_pair in self.pattern2pattern:\n",
    "                overlap = self.pattern2pattern[pattern_pair]\n",
    "            else:\n",
    "                overlap = set()\n",
    "        else:\n",
    "            print('Error: you need to define the method as either soft or hard')\n",
    "            return\n",
    "        for (num,span) in self.pattern2span[pat_idx][1]:\n",
    "            n = '%d_%d'%(pat_idx,num)\n",
    "            if not n in overlap:\n",
    "                diff.append(self.string[max([span[0]-context,0]):min([span[1]+context,len(self.string)])])\n",
    "        diff2 = []\n",
    "        for num,span in self.pattern2span[pat_idx2][1]:\n",
    "            n = '%d_%d'%(pat_idx2,num)\n",
    "            if not n in overlap:\n",
    "                diff2.append(self.string[max([span[0]-context,0]):min([span[1]+context,len(self.string)])])\n",
    "        print('''Found %d overlaps between the expressions:\n",
    "        pattern1: %s \\t and\n",
    "        pattern2: %s\n",
    "        %d included in pattern1 and not in the pattern2\n",
    "        %d was included in pattern2 and not in pattern1'''%(len(overlap),pattern,pattern2,len(diff),len(diff2)))\n",
    "        return diff,diff2\n",
    "    def update_spans(self):\n",
    "        \"Updates matches if a new string is defined.\"\n",
    "        self.pattern2span = []\n",
    "        patterns = list(self.pattern2chars_matched)\n",
    "        self.pattern2chars_matched = {}\n",
    "        for pattern in self.patterns:\n",
    "            self.get_spans(pattern)\n",
    "    def define_string_sample(self,string):\n",
    "        \"Defines and updates the string to explore matches with.\"\n",
    "        self.string = string\n",
    "        self.update_spans()\n",
    "    def create_similarity_matrix(self,method='hard'):\n",
    "        \"Creates a directed similarity matrix between patterns defined.\"\n",
    "        self.make_overlap_network()\n",
    "        pat2n = self.pattern2n_match\n",
    "        patterns = [i[0] for i in self.pattern2span]\n",
    "        #if len(self.similarity_matrix) == len(patterns): # check if it is already defined.\n",
    "         #   return None\n",
    "        if method =='soft':\n",
    "            g = self.pattern2pattern_soft\n",
    "            if len(self.similarity_matrix_soft)==len(self.patterns):\n",
    "                return\n",
    "        else:\n",
    "            if len(self.similarity_matrix)==len(self.patterns):\n",
    "                return\n",
    "            g = self.pattern2pattern\n",
    "        mat = np.empty((len(patterns),len(patterns)))\n",
    "        mat[:] = np.nan\n",
    "        for i in range(len(patterns)-1):\n",
    "            n = self.pattern2n_match[patterns[i]]\n",
    "\n",
    "            for j in range(i+1,len(patterns)):\n",
    "                n2 = self.pattern2n_match[patterns[j]]\n",
    "                pattern_pair = (i,j)\n",
    "                try:\n",
    "                    overlap = len(g[pattern_pair])/2\n",
    "                except:\n",
    "                    overlap = 0\n",
    "                #sum_ = n+n2 - overlap\n",
    "                #try:\n",
    "                #    sim = overlap/sum_\n",
    "                #except:\n",
    "                #    sim = np.nan\n",
    "                if n>0:\n",
    "                    sim = overlap/n\n",
    "                else:\n",
    "                    sim = np.nan\n",
    "                mat[i][j] = sim\n",
    "                if n2>0:\n",
    "                    sim = overlap/n2\n",
    "                else:\n",
    "                    sim = np.nan\n",
    "                mat[j][i] = sim\n",
    "        if method=='soft':\n",
    "            self.similarity_matrix_soft = mat\n",
    "        if method=='hard':\n",
    "            self.similarity_matrix = mat\n",
    "    def plot_similarity(self,method='hard'):\n",
    "        \"\"\"Plots a directed similarity matrix between patterns.\n",
    "        The similarity is defined as number of overlapping matches divided by number of matches.\n",
    "        The definition of overlapping matches between two patterns can be changed from hard (only exact matches) to soft (matches has overlap),\n",
    "        This will allow you to investigate two different things:\n",
    "            * Using the 'hard' method you can see how patterns\n",
    "            * Using 'soft' you can see how expressions narrows the number of accepted patterns.\n",
    "        method: str ['hard','soft'] parameter for defining overlap between regular expression matches. 'hard' entails exact match, and 'soft' defines match as an overlap between matches.\n",
    "         \"\"\"\n",
    "        patterns = self.patterns\n",
    "        self.create_similarity_matrix(method)\n",
    "        if method=='soft':\n",
    "            mat = self.similarity_matrix_soft\n",
    "        else:\n",
    "            mat = self.similarity_matrix\n",
    "        plt.figure(figsize=(12,8))\n",
    "        sns.heatmap(mat,cmap='viridis')\n",
    "        plt.xticks(np.arange(len(patterns))+.5,patterns,rotation=45)\n",
    "        plt.yticks(np.arange(len(patterns))+.5,patterns,rotation=0)\n",
    "        plt.title('Similarity Matrix')\n",
    "    def report(self,method='hard',plot=True):\n",
    "        \"\"\"Report the number of matches of each pattern developed and plot a similarity matrix between them.\n",
    "        The similarity is defined as number of overlapping matches divided by number of matches.\n",
    "        The definition of overlapping matches between two patterns can be changed from hard (only exact matches) to soft (matches has overlap),\n",
    "        This will allow you to investigate two different things:\n",
    "            * Using the 'hard' method you can see how patterns\n",
    "            * Using 'soft' you can see how expressions narrows the number of accepted patterns.\n",
    "        method: str ['hard','soft'] parameter for defining overlap between regular expression matches. 'hard' entails exact match, and 'soft' defines match as an overlap between matches.\n",
    "        \"\"\"\n",
    "        for pattern,n in self.pattern2n_match.items():\n",
    "            print('------ Pattern: %s\\t Matched %d patterns -----' %(pattern,n))\n",
    "        if plot:\n",
    "            self.plot_similarity(method)\n",
    "    def compile_pattern(self,pattern):\n",
    "        \"\"\"Method to compile the final pattern using the default flags set\"\"\"\n",
    "        return re.compile(pattern,flags=self.flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Pattern: kr\t Matched 0 patterns -----\n",
      "------ Pattern: kr\t Matched 0 patterns -----\n",
      "Found 0 overlaps between the expressions:\n",
      "        pattern1: kr \t and\n",
      "        pattern2: kr\n",
      "        0 included in pattern1 and not in the pattern2\n",
      "        0 was included in pattern2 and not in pattern1\n",
      "------ Pattern: [0-9]+kr\t Matched 0 patterns -----\n",
      "------ Pattern: kr\t Matched 0 patterns -----\n",
      "Found 0 overlaps between the expressions:\n",
      "        pattern1: [0-9]+kr \t and\n",
      "        pattern2: kr\n",
      "        0 included in pattern1 and not in the pattern2\n",
      "        0 was included in pattern2 and not in pattern1\n",
      "------ Pattern: [0-9]+(?:[,.][0-9]+)?kr\t Matched 0 patterns -----\n",
      "------ Pattern: kr\t Matched 0 patterns -----\n",
      "Found 0 overlaps between the expressions:\n",
      "        pattern1: [0-9]+(?:[,.][0-9]+)?kr \t and\n",
      "        pattern2: kr\n",
      "        0 included in pattern1 and not in the pattern2\n",
      "        0 was included in pattern2 and not in pattern1\n",
      "------ Pattern: [0-9]+(?:[,.][0-9]+)?\\s{0,2}kr\t Matched 0 patterns -----\n",
      "------ Pattern: kr\t Matched 0 patterns -----\n",
      "Found 0 overlaps between the expressions:\n",
      "        pattern1: [0-9]+(?:[,.][0-9]+)?\\s{0,2}kr \t and\n",
      "        pattern2: kr\n",
      "        0 included in pattern1 and not in the pattern2\n",
      "        0 was included in pattern2 and not in pattern1\n",
      "------ Pattern: [0-9]+(?:[,.][0-9]+)?\\s{0,5}kr(?:oner)?\t Matched 0 patterns -----\n",
      "------ Pattern: kr\t Matched 0 patterns -----\n",
      "Found 0 overlaps between the expressions:\n",
      "        pattern1: [0-9]+(?:[,.][0-9]+)?\\s{0,5}kr(?:oner)? \t and\n",
      "        pattern2: kr\n",
      "        0 included in pattern1 and not in the pattern2\n",
      "        0 was included in pattern2 and not in pattern1\n",
      "------ Pattern: [0-9]+kr\t Matched 0 patterns -----\n"
     ]
    }
   ],
   "source": [
    "explore_money = ExploreRegex(sample_string)\n",
    "\n",
    "first = 'kr'\n",
    "second = '[0-9]+kr'\n",
    "third = '[0-9]+(?:[,.][0-9]+)?kr'\n",
    "fourth = '[0-9]+(?:[,.][0-9]+)?\\s{0,2}kr'\n",
    "final = '[0-9]+(?:[,.][0-9]+)?\\s{0,5}kr(?:oner)?'\n",
    "\n",
    "patterns = [first,second,third,fourth,final]\n",
    "\n",
    "for pattern in patterns:\n",
    "    explore_money.explore_difference(pattern,patterns[0])\n",
    "\n",
    "explore_money.explore_pattern(second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API > Scraping\n",
    "- Use the API if provided\n",
    "- Create a developer account\n",
    "- Learn how to Authenticate by reading their documentation\n",
    "- Construct your queries and collec the data\n",
    "\n",
    "**Examples**\n",
    "- Twitter, YouTube, Reddit, Facebook, Github, Stackexchange, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting something into production?\n",
    "Right now you are working locally on your computer. That is fine for small project, but some might become data scientist working in organizations that would like to put your work into production. Then we need to be really good at version control (git) but also working with servers and databases. This is obviously not part of this course but you might still want to check [RunDeck ](https://www.rundeck.com/open-source) out. It is a open-source tool you can use to orchestra your scripts. Let's say you want your scraper to run every day at 08:00 am, then you can schedule that job in RunDeck or similar tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Junior Data Scientist Position\n",
    "A friend of mine, who is a business owner, is looking to hire a junior data scientist. Sent me two or three of the following documents and you might end up getting a kewl job.\n",
    "1. Code that collects Popular Times data from [this url](https://www.google.com/search?sxsrf=ALeKk02OxWY1MFf29v-44s8Bnozf6EJyHA%3A1597134428470&ei=XFYyX6OTHK_qrgS714u4Ag&q=torvehallerne&oq=torvehallerne&gs_lcp=CgZwc3ktYWIQAzIOCC4QxwEQrwEQywEQkwIyCwguEMcBEK8BEMsBMgsILhDHARCvARDLATILCC4QxwEQrwEQywEyCwguEMcBEK8BEMsBMgsILhDHARCvARDLATIFCAAQywEyCwguEMcBEK8BEMsBMgsILhDHARCvARDLATIFCAAQywE6BwguECcQkwI6BAgAEEM6BwgAEBQQhwI6AggAOg0ILhDHARCvARAnEJMCOgQIIxAnOgUILhCRAjoFCAAQkQI6CAguEMcBEKMCOgoILhDHARCvARBDOgIILjoICC4QxwEQrwE6CwguEMcBEK8BEJMCUJKQAVismgFglZsBaABwAHgBgAGuAYgB7AySAQM0LjmYAQCgAQGqAQdnd3Mtd2l6wAEB&sclient=psy-ab&ved=0ahUKEwijsZGy3ZLrAhUvtYsKHbvrAicQ4dUDCAw&uact=5). Stores and other public places has something called Popular Times when you google them. It indicates how many peoples geolokation are at the place in diffrent time interval. collects data from [this url](https://www.linkedin.com/in/nicklasjohansen/). \n",
    "2. Code that collects LinkedIn data from this url together with a simple idea of how you would use LinkedIn data for a social data science project.\n",
    "3. your_resume.pdf\n",
    "\n",
    "Reach me by nj@sodas.ku.dk. Deadline is september 1st.\n",
    "\n",
    "\n",
    "# Master's Thesis Collaboration\n",
    "I am funded by the [HOPE Project](https://politicalscience.ku.dk/research/projects/hope/) investigating **H**ow Democracies C**ope** with Covid19. We offer thesis collaboration until summer 2023 for student who consider a career in academia. Reach out if you have an idea for your thesis that are within the scope of HOPE.  \n",
    "\n",
    "Reach me by nj@sodas.ku.dk to start a dialogue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "# Exercise Set 8: Web Scraping 3\n",
    "\n",
    "In this Exercise Set we shall develop our webscraping skills even further by practicing using `Selenium` while   parsing and navigating html trees using `BeautifoulSoup`. Furthermore we will train extracting information from raw text with no html tags to help, using regex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 8.1: Translating domains into companies\n",
    "This exercise is about solving a problem that danish companies are facing. They all want to use external data such as customer review data to gain more knowledge about their customers and maybe even use the information as features in their models. There is just one problem: users often create reviews for domains (brand name) and not companies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 8.1.1:** You work for the danish authorities and are currently staffed to a project where you have to reduce the amount of dangerous toys. You have build a webscraper that collect user reviews form Trustpilot and have identified some websites that got a bad reputation among its users. You belive that the risk of them selling illegal or dangerous toys might be bigger than some of the bg brands with good ratings and decide to investigate them. \n",
    "\n",
    "> Go to the website https://www.dk-hostmaster.dk/da/find-domaenenavn with selenium and search for \"netbaby.dk\". Store the name of the registrant \"Euphemia Media\" in the variable `company`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Answer 8.1.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 8.1.2:** Now you know who owns the domain and would like to know more about the company `euphemia media`. \n",
    "\n",
    "> Go to the Central Business Register website https://datacvr.virk.dk/data/. Figure out how to look up companies by changing the url and then lookup `euphemia media`. Store the CVR number in the variable `cvr` and print it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 8.1.3:** Congratulations. You are now able to translate domains into companies and by that enrich what ever analysis you want to make. Let's say that you were to build a scraper who could translate thousands of domains. What kind of errors can you imagine running into and how would you mitigate them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 8.2: Practicing Regular Expressions.\n",
    "This exercise is about developing your experience with designing your own regular expressions. This is especially relevant for those of you who are going to work with text data in your exams.\n",
    "\n",
    "Remember you can always consult the regular expression reference page [here](https://www.regular-expressions.info/refquick.html), if you need to remember or understand a specific symbol. \n",
    "\n",
    "We will use a sample of the trustpilot dataset that you practiced collecting in module_7.\n",
    "You can load it directly into python from the following link: https://raw.githubusercontent.com/snorreralund/scraping_seminar/master/english_review_sample.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 8.2.1:** Load the data used in the exercise using the `pd.read_csv` function. (Hint: path to file can be both a url or systempath). \n",
    "\n",
    ">Define a variable `sample_string = '\\n'.join(df.sample(2000).reviewBody)` as sample of all the reviews that you will practice on.  (Run it once in a while to get a new sample for potential differences).\n",
    "Imagine we were a company wanting to find the reviews where customers are concerned with the price of a service. They decide to write a regular expression to match all reviews where a currencies and an amount is mentioned. \n",
    "\n",
    "> **Ex. 8.2.2:** \n",
    "> Write an expression that matches both the dollar-sign (\\$) and dollar written literally, and the amount before or after a dollar-sign. Remember that the \"$\"-sign is a special character in regular expressions. Explore and refine using the explore_pattern function in the package I created called explore_regex. \n",
    "```python\n",
    "import explore_regex as e_re\n",
    "explore_regex = e_re.Explore_Regex(sample_string) # Initaizlie the Explore regex Class.\n",
    "explore_regex.explore_pattern(pattern) # Use the .explore_pattern method.\n",
    "```\n",
    "\n",
    "\n",
    "Start with exploring the context around digits (\"\\d\") in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex.8.2.3** Use the .report() method. e_re.report(), and print the all patterns in the development process using the .pattern method - i.e. e_re.patterns \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 8.2.4** \n",
    "Finally write a function that takes in a string and outputs if there is a match. Use the .match function to see if there is a match (hint if does not return a NoneType object - `re.match(pattern,string)!=None`).\n",
    "\n",
    "> Define a column 'mention_currency' in the dataframe, by applying the above function to the text column of the dataframe. \n",
    "*** You should have approximately 310 reviews that matches. - but less is also alright***\n",
    "\n",
    "> **Ex. 8.2.5** Explore the relation between reviews mentioning prices and the average rating. \n",
    "\n",
    "> **Ex. 8.2.6 (extra)** Define a function that outputs the amount mentioned in the review (if more than one the largest), define a new column by applying it to the data, and explore whether reviews mentioning higher prices are worse than others by plotting the amount versus the rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 8.2.7:** Now we write a regular expression to extract emoticons from text.\n",
    "Start by locating all mouths ')' of emoticons, and develop the variations from there. Remember that paranthesis are special characters in regex, so you should use the escape character."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "nav_menu": {
    "height": "328px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
